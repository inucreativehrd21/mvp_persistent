# ============================================================================
# vLLM 서버 설정
# ============================================================================

# 사용할 모델 (HuggingFace 모델 ID)
# 권장: Qwen/Qwen2.5-Coder-7B-Instruct (8GB VRAM)
# 기타: deepseek-ai/deepseek-coder-7b-instruct-v1.5, codellama/CodeLlama-7b-Instruct-hf
VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct

# vLLM 서버 포트 (기본: 8000)
VLLM_PORT=8000

# vLLM 서버 URL (클라이언트에서 접속할 주소)
VLLM_SERVER_URL=http://localhost:8000/v1

# 최대 컨텍스트 길이 (모델에 따라 조정)
MAX_MODEL_LEN=4096

# GPU 메모리 사용률 (0.0~1.0, 기본: 0.9)
GPU_MEMORY_UTIL=0.9

# ============================================================================
# HuggingFace 설정
# ============================================================================

# HuggingFace 캐시 디렉토리 (모델 다운로드 위치)
# Linux/Mac: ~/.cache/huggingface
# Windows: C:\Users\YourName\.cache\huggingface
HUGGINGFACE_CACHE_DIR=~/.cache/huggingface

# HuggingFace 토큰 (Private 모델 사용 시 필요, 선택사항)
# https://huggingface.co/settings/tokens 에서 발급
HUGGING_FACE_HUB_TOKEN=

# ============================================================================
# 애플리케이션 설정
# ============================================================================

# Gradio 서버 포트 (기본: 7860)
GRADIO_PORT=7860

# Gradio 서버 호스트 (RunPod 등 외부 접속 시 0.0.0.0)
GRADIO_HOST=127.0.0.1

# 데이터 파일 경로 (문제 데이터)
DATA_FILE_PATH=./data/problems_multi_solution.json

# ============================================================================
# 추가 설정 (선택사항)
# ============================================================================

# 로그 레벨 (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# 로그 파일 경로
LOG_FILE_PATH=./logs/app.log
