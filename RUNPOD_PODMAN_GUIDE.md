# RunPod Explore Pod + Podman ë°°í¬ ê°€ì´ë“œ

**ë°°í¬ ë°©ì‹:** RunPod Explore Pod(Docker ë°ëª¬ ì—†ëŠ” í™˜ê²½)ì—ì„œ Podmanìœ¼ë¡œ vLLM ì»¨í…Œì´ë„ˆ ì‹¤í–‰

## ğŸ¯ ì´ ë°©ì‹ì´ë€?

```
RunPod Explore Pod (GPU ì¸ìŠ¤í„´ìŠ¤, Docker ë°ëª¬ âŒ)
â””â”€â”€ Podman Container (Docker ëŒ€ì²´, ë°ëª¬ ë¶ˆí•„ìš”)
    â””â”€â”€ vLLM ì„œë²„
        â””â”€â”€ Qwen 7B ëª¨ë¸
```

**íŠ¹ì§•:**
- âœ… Docker ë°ëª¬ ì—†ì´ ì»¨í…Œì´ë„ˆ ì‹¤í–‰
- âœ… docker-compose.yml í˜¸í™˜
- âœ… ê¸°ì¡´ Docker ì›Œí¬í”Œë¡œìš° ìœ ì§€
- âœ… ë©˜í† ë‹˜ì´ ì¶”ì²œí•œ Docker ì´ë¯¸ì§€ ë°©ì‹ ê·¸ëŒ€ë¡œ

**Podman vs Docker:**
- Podman: ë°ëª¬ ì—†ì´ ì‘ë™ (Daemonless)
- Docker: ë°ëª¬ í•„ìš” (ëŸ°íŒŸ Explore Podì—ì„œ ë¶ˆê°€)
- CLIëŠ” ê±°ì˜ ë™ì¼: `docker` â†’ `podman`

---

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„

### 1. RunPod ê³„ì •
- [RunPod](https://www.runpod.io/) ê°€ì…
- ê²°ì œ ìˆ˜ë‹¨ ë“±ë¡

### 2. ë¡œì»¬ í™˜ê²½ (í”„ë¡œì íŠ¸ ì—…ë¡œë“œìš©)
- Git ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ ì¤€ë¹„

---

## ğŸš€ ë‹¨ê³„ë³„ ë°°í¬

### 1ë‹¨ê³„: RunPod Explore Pod ìƒì„±

#### 1.1 RunPod ëŒ€ì‹œë³´ë“œ ì ‘ì†
- **Pods** ë©”ë‰´ í´ë¦­
- **+ Deploy** í´ë¦­

#### 1.2 GPU ì„ íƒ
```
ê¶Œì¥ GPU (ëª¨ë¸ë³„):
- Qwen 7B: RTX 3090 (24GB) ì´ìƒ
- Qwen 14B: RTX 4090 (24GB) ë˜ëŠ” A100 (40GB)
```

#### 1.3 í…œí”Œë¦¿ ì„ íƒ
- **RunPod PyTorch** ì„ íƒ
- ì´ë¯¸ Python, CUDA, PyTorch ì‚¬ì „ ì„¤ì¹˜ë¨

#### 1.4 ìŠ¤í† ë¦¬ì§€ ì„¤ì •
- **Container Disk:** 20GB (ëª¨ë¸ ìºì‹œìš©)
- **Volume Disk:** 50GB (ì˜êµ¬ ë°ì´í„°ìš©, ì„ íƒ)

#### 1.5 ë„¤íŠ¸ì›Œí¬ ì„¤ì •
- **Expose HTTP Ports:** `8000, 7860` ì¶”ê°€
  - 8000: vLLM API ì„œë²„
  - 7860: Gradio UI

#### 1.6 ë°°í¬
- **Deploy** í´ë¦­
- Pod ì‹œì‘ ëŒ€ê¸° (1~2ë¶„)

---

### 2ë‹¨ê³„: Pod ì ‘ì†

#### SSH ì ‘ì†
```bash
# RunPod ëŒ€ì‹œë³´ë“œì—ì„œ SSH ëª…ë ¹ ë³µì‚¬
ssh root@<pod-id>.ssh.runpod.io -p <port>
```

#### ë˜ëŠ” Web Terminal
- RunPod ëŒ€ì‹œë³´ë“œì—ì„œ **Terminal** í´ë¦­
- ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì ‘ì†

---

### 3ë‹¨ê³„: Podman ì„¤ì¹˜

```bash
# ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
apt-get update

# Podman ì„¤ì¹˜
apt-get install -y podman

# ì„¤ì¹˜ í™•ì¸
podman --version
# ì¶œë ¥ ì˜ˆì‹œ: podman version 3.4.4
```

---

### 4ë‹¨ê³„: nvidia-container-toolkit ì„¤ì •

```bash
# NVIDIA Container Toolkit ì„¤ì¹˜ (GPU ì ‘ê·¼ìš©)
apt-get install -y nvidia-container-toolkit

# Podmanì´ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ì •
# ë°©ë²• 1: CDI (Container Device Interface) ìƒì„±
nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml

# ë°©ë²• 2: crun ëŸ°íƒ€ì„ ì„¤ì • (ìœ„ ë°©ë²•ì´ ì•ˆ ë˜ë©´)
nvidia-ctk runtime configure --runtime=crun --config=/usr/share/containers/containers.conf

# GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸
podman run --rm --device nvidia.com/gpu=all docker.io/nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

**ì •ìƒ ì¶œë ¥:**
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.xx.xx    Driver Version: 525.xx.xx    CUDA Version: 12.0   |
+-----------------------------------------------------------------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
...
```

**ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´ ë°©ë²•:**
```bash
# GPU ë””ë°”ì´ìŠ¤ë¥¼ ì§ì ‘ ë§ˆìš´íŠ¸ (docker-compose.podman.ymlì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)
podman run --rm \
  --device /dev/nvidia0:/dev/nvidia0 \
  --device /dev/nvidiactl:/dev/nvidiactl \
  --device /dev/nvidia-uvm:/dev/nvidia-uvm \
  docker.io/nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

---

### 5ë‹¨ê³„: podman-compose ì„¤ì¹˜

```bash
# podman-compose ì„¤ì¹˜ (docker-compose ëŒ€ì²´)
pip install podman-compose

# ì„¤ì¹˜ í™•ì¸
podman-compose --version
# ì¶œë ¥ ì˜ˆì‹œ: podman-compose version 1.0.6
```

---

### 6ë‹¨ê³„: í”„ë¡œì íŠ¸ ì—…ë¡œë“œ

#### ë°©ë²• A: Git Clone (ê¶Œì¥)
```bash
cd /workspace
git clone https://github.com/your-username/mvp_persistent.git
cd mvp_persistent
```

#### ë°©ë²• B: íŒŒì¼ ì—…ë¡œë“œ
```bash
# ë¡œì»¬ì—ì„œ í”„ë¡œì íŠ¸ ì••ì¶•
cd C:\develop1
tar -czf mvp_persistent.tar.gz mvp_persistent/

# RunPod Podìœ¼ë¡œ ì „ì†¡
scp -P <port> mvp_persistent.tar.gz root@<pod-id>.ssh.runpod.io:/workspace/

# Podì—ì„œ ì••ì¶• í•´ì œ
cd /workspace
tar -xzf mvp_persistent.tar.gz
cd mvp_persistent
```

---

### 7ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
cd /workspace/mvp_persistent

# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# í¸ì§‘
nano .env
```

**RunPod Explore Pod ì „ìš© .env ì„¤ì •:**
```env
# vLLM ëª¨ë¸
VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct

# í¬íŠ¸
VLLM_PORT=8000
GRADIO_PORT=7860

# ì„œë²„ URL (Pod ë‚´ë¶€)
VLLM_SERVER_URL=http://localhost:8000/v1

# Gradio ì™¸ë¶€ ì ‘ì† í—ˆìš© (ì¤‘ìš”!)
GRADIO_HOST=0.0.0.0

# HuggingFace ìºì‹œ (Pod ì˜êµ¬ ìŠ¤í† ë¦¬ì§€)
HUGGINGFACE_CACHE_DIR=/workspace/.cache/huggingface

# GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©
GPU_MEMORY_UTIL=0.95
MAX_MODEL_LEN=4096

# (ì„ íƒ) HuggingFace í† í°
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxx
```

---

### 8ë‹¨ê³„: vLLM Podman ì„œë²„ ì‹œì‘

```bash
# Podman ì „ìš© ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
chmod +x start_server_podman.sh
./start_server_podman.sh
```

**ì¶œë ¥:**
```
ğŸš€ vLLM ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤... (Podman)
   ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

GPU ìƒíƒœ í™•ì¸ ì¤‘...
NVIDIA GeForce RTX 3090, 24576 MiB

âœ… Podman í™˜ê²½ í™•ì¸ ì™„ë£Œ

ğŸš€ vLLM Podman ì»¨í…Œì´ë„ˆë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...

[+] Running 1/1
 âœ” Container vllm-hint-server  Started

âœ… vLLM ì„œë²„ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!
```

#### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëª¨ë‹ˆí„°ë§
```bash
# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
podman-compose -f docker-compose.podman.yml logs -f vllm-server

# ë˜ëŠ” podman ì§ì ‘ ì‚¬ìš©
podman logs -f vllm-hint-server

# "INFO: Application startup complete" ì¶œë ¥ ëŒ€ê¸°
```

**ì²« ì‹¤í–‰ ì‹œê°„:**
- Qwen 7B: ì•½ 5~10ë¶„ (14GB ë‹¤ìš´ë¡œë“œ)
- Qwen 14B: ì•½ 10~15ë¶„ (28GB ë‹¤ìš´ë¡œë“œ)

---

### 9ë‹¨ê³„: ì„œë²„ ìƒíƒœ í™•ì¸

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ
podman-compose -f docker-compose.podman.yml ps

# ë˜ëŠ”
podman ps

# Health check
curl http://localhost:8000/health

# ëª¨ë¸ í™•ì¸
curl http://localhost:8000/v1/models
```

**ì •ìƒ ì¶œë ¥:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "object": "model",
      "created": 1234567890,
      "owned_by": "vllm"
    }
  ]
}
```

---

### 10ë‹¨ê³„: Gradio í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰

#### ìƒˆ í„°ë¯¸ë„ ì—´ê¸° (tmux ê¶Œì¥)
```bash
# tmux ì„¸ì…˜ ìƒì„±
tmux new -s gradio

# í´ë¼ì´ì–¸íŠ¸ ì‹œì‘
cd /workspace/mvp_persistent
./start_client.sh
```

**ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰:**
```bash
nohup ./start_client.sh > gradio.log 2>&1 &
```

---

### 11ë‹¨ê³„: ì™¸ë¶€ ì ‘ì†

RunPodì€ ìë™ìœ¼ë¡œ í¬íŠ¸ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.

#### 11.1 RunPod ëŒ€ì‹œë³´ë“œì—ì„œ URL í™•ì¸
- Pod ìƒì„¸ í˜ì´ì§€
- **TCP Port Mappings** ì„¹ì…˜
- í¬íŠ¸ 7860ê³¼ 8000ì˜ Public URL ë³µì‚¬

#### 11.2 ì ‘ì†
```
Gradio UI: https://<pod-id>-7860.proxy.runpod.net
vLLM API: https://<pod-id>-8000.proxy.runpod.net
```

---

## ğŸ”§ ê´€ë¦¬ ëª…ë ¹ì–´ (Podman)

### ì„œë²„ ê´€ë¦¬
```bash
# ì„œë²„ ì¤‘ì§€
podman-compose -f docker-compose.podman.yml down

# ì„œë²„ ì¬ì‹œì‘
podman-compose -f docker-compose.podman.yml restart

# ë¡œê·¸ í™•ì¸
podman-compose -f docker-compose.podman.yml logs -f vllm-server

# ë˜ëŠ” podman ì§ì ‘ ì‚¬ìš©
podman logs -f vllm-hint-server

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
podman exec -it vllm-hint-server bash

# ì»¨í…Œì´ë„ˆ ìƒíƒœ
podman ps -a
```

### ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
```bash
# GPU ì‚¬ìš©ë¥ 
watch -n 1 nvidia-smi

# ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤
podman stats vllm-hint-server
```

### ì´ë¯¸ì§€ ê´€ë¦¬
```bash
# ì´ë¯¸ì§€ ëª©ë¡
podman images

# ì´ë¯¸ì§€ ì—…ë°ì´íŠ¸ (vLLM ìƒˆ ë²„ì „)
podman pull vllm/vllm-openai:latest
podman-compose -f docker-compose.podman.yml down
podman-compose -f docker-compose.podman.yml up -d

# ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì‚­ì œ
podman image prune -a
```

### ëª¨ë¸ ìºì‹œ ê´€ë¦¬
```bash
# ìºì‹œ ìœ„ì¹˜ í™•ì¸
ls -lh /workspace/.cache/huggingface/hub

# ìºì‹œ í¬ê¸° í™•ì¸
du -sh /workspace/.cache/huggingface/hub

# ìºì‹œ ì‚­ì œ (ì¬ë‹¤ìš´ë¡œë“œë¨)
rm -rf /workspace/.cache/huggingface/hub
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. "GPU ì ‘ê·¼ ë¶ˆê°€" ì˜¤ë¥˜
```bash
# ë°©ë²• 1: CDI ì¬ìƒì„±
nvidia-ctk cdi generate --output=/etc/cdi/nvidia.yaml

# ë°©ë²• 2: GPU ë””ë°”ì´ìŠ¤ í™•ì¸
ls -l /dev/nvidia*

# ë°©ë²• 3: docker-compose.podman.ymlì—ì„œ devices ì§ì ‘ ì§€ì • (ì´ë¯¸ ì„¤ì •ë¨)
# devices:
#   - /dev/nvidia0:/dev/nvidia0
#   - /dev/nvidiactl:/dev/nvidiactl
#   - /dev/nvidia-uvm:/dev/nvidia-uvm

# ì„œë²„ ì¬ì‹œì‘
podman-compose -f docker-compose.podman.yml restart
```

### 2. "Out of Memory"
```bash
# .envì—ì„œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë‚®ì¶”ê¸°
GPU_MEMORY_UTIL=0.8  # 0.95 â†’ 0.8

# ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶•ì†Œ
MAX_MODEL_LEN=2048  # 4096 â†’ 2048

# ì„œë²„ ì¬ì‹œì‘
podman-compose -f docker-compose.podman.yml restart
```

### 3. "Connection refused"
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
podman ps

# ë¡œê·¸ í™•ì¸
podman logs vllm-hint-server

# í¬íŠ¸ í™•ì¸
netstat -tlnp | grep 8000

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ health check
podman exec vllm-hint-server curl http://localhost:8000/health
```

### 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼
```bash
# HuggingFace í† í° ì„¤ì •
export HUGGING_FACE_HUB_TOKEN="hf_xxxxx"

# ë˜ëŠ” .env íŒŒì¼ ìˆ˜ì •
nano .env
# HUGGING_FACE_HUB_TOKEN=hf_xxxxx

# ì„œë²„ ì¬ì‹œì‘
podman-compose -f docker-compose.podman.yml restart
```

### 5. Podman ì„¤ì¹˜ ì‹¤íŒ¨
```bash
# ì €ì¥ì†Œ ì¶”ê°€
. /etc/os-release
echo "deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /" | \
  tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list

curl -L "https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/Release.key" | \
  apt-key add -

apt-get update
apt-get install -y podman
```

### 6. "runtime not found" ì˜¤ë¥˜
```bash
# crun ëŸ°íƒ€ì„ ì„¤ì¹˜
apt-get install -y crun

# containers.conf í™•ì¸
cat /usr/share/containers/containers.conf | grep runtime

# nvidia-container-toolkit ì¬ì„¤ì •
nvidia-ctk runtime configure --runtime=crun --config=/usr/share/containers/containers.conf

# Podman ì¬ì‹œì‘
systemctl --user restart podman  # rootlessì¸ ê²½ìš°
# ë˜ëŠ”
killall podman  # rootì¸ ê²½ìš°
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

| í•­ëª© | Docker (ë°ëª¬) | Podman (ë°ëª¬ë¦¬ìŠ¤) |
|------|--------------|------------------|
| **ì‹œì‘ ì†ë„** | ë¹ ë¦„ | ì•½ê°„ ëŠë¦¼ |
| **ë©”ëª¨ë¦¬** | ì•½ê°„ ë†’ìŒ | ë‚®ìŒ |
| **ë³´ì•ˆ** | ë£¨íŠ¸ í•„ìš” | Rootless ê°€ëŠ¥ |
| **í˜¸í™˜ì„±** | ë†’ìŒ | ë†’ìŒ (95%+) |
| **ëŸ°íŒŸ Explore Pod** | âŒ ë¶ˆê°€ | âœ… ê°€ëŠ¥ |

---

## ğŸ†š Docker vs Podman ëª…ë ¹ì–´ ë¹„êµ

| Docker | Podman | ì„¤ëª… |
|--------|--------|------|
| `docker-compose up -d` | `podman-compose -f docker-compose.podman.yml up -d` | ì»¨í…Œì´ë„ˆ ì‹œì‘ |
| `docker-compose down` | `podman-compose -f docker-compose.podman.yml down` | ì»¨í…Œì´ë„ˆ ì¤‘ì§€ |
| `docker ps` | `podman ps` | ì»¨í…Œì´ë„ˆ ëª©ë¡ |
| `docker logs <container>` | `podman logs <container>` | ë¡œê·¸ í™•ì¸ |
| `docker exec -it <container> bash` | `podman exec -it <container> bash` | ì»¨í…Œì´ë„ˆ ì ‘ì† |

**ê±°ì˜ ë™ì¼!** `docker` â†’ `podman`ë§Œ ë°”ê¾¸ë©´ ë©ë‹ˆë‹¤.

---

## ğŸ’° ë¹„ìš© ìµœì í™”

### 1. Pod ìë™ ì¤‘ì§€
RunPod ëŒ€ì‹œë³´ë“œì—ì„œ:
- **Auto Stop** ì„¤ì •
- ìœ íœ´ ì‹œê°„ í›„ ìë™ ì¤‘ì§€

### 2. Spot Instances
- **Community Cloud** ì„ íƒ
- ìµœëŒ€ 70% ì €ë ´
- ë‹¨, ì–¸ì œë“  ì¢…ë£Œ ê°€ëŠ¥

### 3. í•„ìš”í•  ë•Œë§Œ ì‹¤í–‰
```bash
# ì‚¬ìš© ì „ ì‹œì‘
./start_server_podman.sh

# ì‚¬ìš© í›„ ì¤‘ì§€
podman-compose -f docker-compose.podman.yml down
```

---

## ğŸ“ ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [Podman ê³µì‹ ë¬¸ì„œ](https://docs.podman.io/)
- [podman-compose GitHub](https://github.com/containers/podman-compose)
- [vLLM Docker ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/en/latest/deployment/docker.html)
- [RunPod ë¬¸ì„œ](https://docs.runpod.io/)

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë°°í¬ ì™„ë£Œ í™•ì¸
- [ ] RunPod Pod ìƒì„± ì™„ë£Œ
- [ ] Podman ì„¤ì¹˜ ì™„ë£Œ
- [ ] podman-compose ì„¤ì¹˜ ì™„ë£Œ
- [ ] nvidia-container-toolkit ì„¤ì • ì™„ë£Œ
- [ ] GPU ì ‘ê·¼ í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] í”„ë¡œì íŠ¸ ì—…ë¡œë“œ ì™„ë£Œ
- [ ] .env íŒŒì¼ ì„¤ì • ì™„ë£Œ
- [ ] vLLM ì„œë²„ ì‹œì‘ ì„±ê³µ
- [ ] ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [ ] Health check í†µê³¼
- [ ] Gradio í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰ ì„±ê³µ
- [ ] ì™¸ë¶€ ì ‘ì† ì„±ê³µ

---

**ì´ ë°©ì‹ì˜ ì¥ì :**
- âœ… ë©˜í† ë‹˜ì´ ì¶”ì²œí•œ Docker ì´ë¯¸ì§€ ë°©ì‹ ìœ ì§€
- âœ… ëŸ°íŒŸ Explore Pod (ë°ëª¬ ì—†ëŠ” í™˜ê²½)ì—ì„œ ì‘ë™
- âœ… docker-compose.yml í˜¸í™˜
- âœ… ê¸°ì¡´ ì›Œí¬í”Œë¡œìš° ë³€ê²½ ìµœì†Œí™”

**Docker vs Podman ì°¨ì´:**
- Docker: ë°ëª¬ í•„ìš”, ëŸ°íŒŸ Explore Pod ë¶ˆê°€
- Podman: ë°ëª¬ ë¶ˆí•„ìš”, ëŸ°íŒŸ Explore Pod ê°€ëŠ¥

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-05
**í…ŒìŠ¤íŠ¸ í™˜ê²½:** RunPod Explore Pod, RTX 3090, Podman 3.4.4
