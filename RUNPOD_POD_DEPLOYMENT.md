# RunPod Pod + Docker ë°°í¬ ê°€ì´ë“œ

**ë°°í¬ ë°©ì‹:** RunPod Pod(ì¸ìŠ¤í„´ìŠ¤)ë¥¼ ìƒì„±í•˜ê³  ê·¸ ì•ˆì—ì„œ Dockerë¡œ vLLM ì„œë²„ë¥¼ ì‹¤í–‰

## ğŸ¯ ì´ ë°©ì‹ì´ë€?

```
RunPod Pod (GPU ì¸ìŠ¤í„´ìŠ¤)
â””â”€â”€ Docker Container (vLLM ì„œë²„)
    â””â”€â”€ Qwen 7B ëª¨ë¸
```

**íŠ¹ì§•:**
- âœ… í•­ìƒ ì‹¤í–‰ ì¤‘ (Persistent)
- âœ… ì¦‰ì‹œ ì‘ë‹µ (< 1ì´ˆ)
- âœ… Dockerë¡œ ê²©ë¦¬ëœ í™˜ê²½
- âœ… ì™„ì „í•œ ì œì–´ê¶Œ

**vs Serverless:**
- Serverless: RunPodì´ ëª¨ë“  ê²ƒ ê´€ë¦¬, ìë™ í™•ì¥
- Pod + Docker: ì§ì ‘ ê´€ë¦¬, ê³ ì • ì„±ëŠ¥

---

## ğŸ“‹ ì‚¬ì „ ì¤€ë¹„

### 1. RunPod ê³„ì •
- [RunPod](https://www.runpod.io/) ê°€ì…
- ê²°ì œ ìˆ˜ë‹¨ ë“±ë¡

### 2. ë¡œì»¬ í™˜ê²½ (í”„ë¡œì íŠ¸ ì—…ë¡œë“œìš©)
- Git ë˜ëŠ” íŒŒì¼ ì—…ë¡œë“œ ì¤€ë¹„
- SSH í‚¤ ìƒì„± (ì„ íƒ)

---

## ğŸš€ ë‹¨ê³„ë³„ ë°°í¬

### 1ë‹¨ê³„: RunPod Pod ìƒì„±

#### 1.1 RunPod ëŒ€ì‹œë³´ë“œ ì ‘ì†
- **Pods** ë©”ë‰´ í´ë¦­
- **+ Deploy** í´ë¦­

#### 1.2 GPU ì„ íƒ
```
ê¶Œì¥ GPU (ëª¨ë¸ë³„):
- Qwen 7B: RTX 3090 (24GB) ì´ìƒ
- Qwen 14B: RTX 4090 (24GB) ë˜ëŠ” A100 (40GB)
- Qwen 32B: A100 (80GB) ë˜ëŠ” 2x A100 (40GB)
```

#### 1.3 í…œí”Œë¦¿ ì„ íƒ
- **RunPod PyTorch** ë˜ëŠ” **RunPod Tensorflow** ì„ íƒ
- Dockerê°€ ì‚¬ì „ ì„¤ì¹˜ë¨
- NVIDIA Container Toolkit í¬í•¨

#### 1.4 ìŠ¤í† ë¦¬ì§€ ì„¤ì •
- **Container Disk:** 20GB (ëª¨ë¸ ìºì‹œìš©)
- **Volume Disk:** 50GB (ì˜êµ¬ ë°ì´í„°ìš©, ì„ íƒ)

#### 1.5 ë„¤íŠ¸ì›Œí¬ ì„¤ì •
- **Expose HTTP Ports:** `8000, 7860` ì¶”ê°€
  - 8000: vLLM API ì„œë²„
  - 7860: Gradio UI

#### 1.6 SSH í‚¤ ì¶”ê°€ (ì„ íƒ)
- ë³¸ì¸ì˜ ê³µê°œ SSH í‚¤ ì¶”ê°€

#### 1.7 ë°°í¬
- **Deploy** í´ë¦­
- Pod ì‹œì‘ ëŒ€ê¸° (1~2ë¶„)

---

### 2ë‹¨ê³„: Pod ì ‘ì†

#### SSH ì ‘ì†
```bash
# RunPod ëŒ€ì‹œë³´ë“œì—ì„œ SSH ëª…ë ¹ ë³µì‚¬
ssh root@<pod-id>.ssh.runpod.io -p <port> -i ~/.ssh/id_ed25519
```

#### ë˜ëŠ” Web Terminal
- RunPod ëŒ€ì‹œë³´ë“œì—ì„œ **Terminal** í´ë¦­
- ë¸Œë¼ìš°ì €ì—ì„œ ë°”ë¡œ ì ‘ì†

---

### 3ë‹¨ê³„: í”„ë¡œì íŠ¸ ì—…ë¡œë“œ

#### ë°©ë²• A: Git Clone (ê¶Œì¥)
```bash
cd /workspace
git clone https://github.com/your-username/final_project.git
cd final_project
```

#### ë°©ë²• B: íŒŒì¼ ì—…ë¡œë“œ
```bash
# ë¡œì»¬ì—ì„œ í”„ë¡œì íŠ¸ ì••ì¶•
cd C:\develop1
tar -czf final_project.tar.gz final_project/

# RunPod Podìœ¼ë¡œ ì „ì†¡
scp -P <port> final_project.tar.gz root@<pod-id>.ssh.runpod.io:/workspace/

# Podì—ì„œ ì••ì¶• í•´ì œ
cd /workspace
tar -xzf final_project.tar.gz
cd final_project
```

#### ë°©ë²• C: Jupyter Lab ì—…ë¡œë“œ
- RunPod ëŒ€ì‹œë³´ë“œì—ì„œ **Jupyter Lab** í´ë¦­
- íŒŒì¼ íƒìƒ‰ê¸°ì—ì„œ ë“œë˜ê·¸ ì•¤ ë“œë¡­
- `/workspace` ë””ë ‰í† ë¦¬ì— ì—…ë¡œë“œ

---

### 4ë‹¨ê³„: Docker ë° í™˜ê²½ í™•ì¸

```bash
# Docker ì„¤ì¹˜ í™•ì¸
docker --version

# NVIDIA Docker ëŸ°íƒ€ì„ í™•ì¸
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi

# ì¶œë ¥ ì˜ˆì‹œ:
# +-----------------------------------------------------------------------------+
# | NVIDIA-SMI 525.xx.xx    Driver Version: 525.xx.xx    CUDA Version: 12.0   |
# ...
```

---

### 5ë‹¨ê³„: í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
cd /workspace/final_project

# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# í¸ì§‘
nano .env
```

**RunPod Pod ì „ìš© .env ì„¤ì •:**
```env
# vLLM ëª¨ë¸
VLLM_MODEL=Qwen/Qwen2.5-Coder-7B-Instruct

# í¬íŠ¸ (ê¸°ë³¸ê°’ ì‚¬ìš©)
VLLM_PORT=8000
GRADIO_PORT=7860

# ì„œë²„ URL (Pod ë‚´ë¶€)
VLLM_SERVER_URL=http://localhost:8000/v1

# Gradio ì™¸ë¶€ ì ‘ì† í—ˆìš©
GRADIO_HOST=0.0.0.0

# HuggingFace ìºì‹œ (Pod ì˜êµ¬ ìŠ¤í† ë¦¬ì§€)
HUGGINGFACE_CACHE_DIR=/workspace/.cache/huggingface

# GPU ë©”ëª¨ë¦¬ ìµœëŒ€ í™œìš©
GPU_MEMORY_UTIL=0.95

# (ì„ íƒ) HuggingFace í† í°
HUGGING_FACE_HUB_TOKEN=hf_xxxxxxxxxxxxx

# Keep-Warm ë¹„í™œì„±í™” (PodëŠ” í•­ìƒ ì‹¤í–‰ ì¤‘)
ENABLE_KEEP_WARM=false
```

---

### 6ë‹¨ê³„: vLLM Docker ì„œë²„ ì‹œì‘

```bash
# ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‹œì‘ (ê¶Œì¥)
./start_server.sh

# ë˜ëŠ” ì§ì ‘ ì‹¤í–‰
docker-compose up -d
```

**ì¶œë ¥:**
```
ğŸš€ vLLM ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...
   ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

[+] Running 1/1
 âœ” Container vllm-hint-server  Started

âœ… vLLM ì„œë²„ê°€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!
```

#### ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëª¨ë‹ˆí„°ë§
```bash
# ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
docker-compose logs -f vllm-server

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í™•ì¸
# "INFO: Application startup complete" ì¶œë ¥ ëŒ€ê¸°
```

**ì²« ì‹¤í–‰ ì‹œê°„:**
- Qwen 7B: ì•½ 5~10ë¶„ (14GB ë‹¤ìš´ë¡œë“œ)
- Qwen 14B: ì•½ 10~15ë¶„ (28GB ë‹¤ìš´ë¡œë“œ)

---

### 7ë‹¨ê³„: ì„œë²„ ìƒíƒœ í™•ì¸

```bash
# ì»¨í…Œì´ë„ˆ ìƒíƒœ
docker-compose ps

# Health check
curl http://localhost:8000/health

# ëª¨ë¸ í™•ì¸
curl http://localhost:8000/v1/models
```

**ì •ìƒ ì¶œë ¥:**
```json
{
  "object": "list",
  "data": [
    {
      "id": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "object": "model",
      ...
    }
  ]
}
```

---

### 8ë‹¨ê³„: Gradio í´ë¼ì´ì–¸íŠ¸ ì‹¤í–‰

#### ìƒˆ í„°ë¯¸ë„ ì—´ê¸° (tmux ê¶Œì¥)
```bash
# tmux ì„¸ì…˜ ìƒì„±
tmux new -s gradio

# í´ë¼ì´ì–¸íŠ¸ ì‹œì‘
cd /workspace/final_project
./start_client.sh
```

**ë˜ëŠ” ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰:**
```bash
nohup ./start_client.sh > gradio.log 2>&1 &
```

---

### 9ë‹¨ê³„: ì™¸ë¶€ ì ‘ì†

RunPodì€ ìë™ìœ¼ë¡œ í¬íŠ¸ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.

#### 9.1 RunPod ëŒ€ì‹œë³´ë“œì—ì„œ URL í™•ì¸
- Pod ìƒì„¸ í˜ì´ì§€
- **TCP Port Mappings** ì„¹ì…˜
- í¬íŠ¸ 7860ê³¼ 8000ì˜ Public URL ë³µì‚¬

#### 9.2 ì ‘ì†
```
Gradio UI: https://<pod-id>-7860.proxy.runpod.net
vLLM API: https://<pod-id>-8000.proxy.runpod.net
```

---

## ğŸ”§ ê´€ë¦¬ ëª…ë ¹ì–´

### ì„œë²„ ê´€ë¦¬
```bash
# ì„œë²„ ì¤‘ì§€
docker-compose down

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart

# ë¡œê·¸ í™•ì¸
docker-compose logs -f vllm-server

# ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ì ‘ì†
docker-compose exec vllm-server bash
```

### ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
```bash
# GPU ì‚¬ìš©ë¥ 
watch -n 1 nvidia-smi

# ì»¨í…Œì´ë„ˆ ë¦¬ì†ŒìŠ¤
docker stats vllm-hint-server
```

### ëª¨ë¸ ìºì‹œ ê´€ë¦¬
```bash
# ìºì‹œ ìœ„ì¹˜ í™•ì¸
ls -lh /workspace/.cache/huggingface/hub

# ìºì‹œ ì‚­ì œ (ì¬ë‹¤ìš´ë¡œë“œë¨)
rm -rf /workspace/.cache/huggingface/hub
```

---

## ğŸ’° ë¹„ìš© ìµœì í™”

### 1. Pod ìë™ ì¤‘ì§€
RunPod ëŒ€ì‹œë³´ë“œì—ì„œ:
- **Auto Stop** ì„¤ì •
- ìœ íœ´ ì‹œê°„ í›„ ìë™ ì¤‘ì§€

### 2. Spot Instances
- **Community Cloud** ì„ íƒ
- ìµœëŒ€ 70% ì €ë ´
- ë‹¨, ì–¸ì œë“  ì¢…ë£Œ ê°€ëŠ¥

### 3. í•„ìš”í•  ë•Œë§Œ ì‹¤í–‰
```bash
# ì‚¬ìš© ì „ ì‹œì‘
docker-compose up -d

# ì‚¬ìš© í›„ ì¤‘ì§€
docker-compose down
```

---

## ğŸ› ë¬¸ì œ í•´ê²°

### 1. "NVIDIA runtime not found"
```bash
# nvidia-container-toolkit ì„¤ì¹˜
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### 2. "Out of Memory"
```bash
# .envì—ì„œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë‚®ì¶”ê¸°
GPU_MEMORY_UTIL=0.8  # 0.95 â†’ 0.8

# ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì¶•ì†Œ
MAX_MODEL_LEN=2048  # 4096 â†’ 2048

# ì„œë²„ ì¬ì‹œì‘
docker-compose restart
```

### 3. "Connection refused"
```bash
# ì„œë²„ ìƒíƒœ í™•ì¸
docker-compose ps

# ë¡œê·¸ í™•ì¸
docker-compose logs vllm-server

# í¬íŠ¸ í™•ì¸
netstat -tlnp | grep 8000
```

### 4. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ëŠë¦¼
```bash
# HuggingFace í† í° ì„¤ì •
export HUGGING_FACE_HUB_TOKEN="hf_xxxxx"

# ë˜ëŠ” .env íŒŒì¼ ìˆ˜ì •
```

---

## ğŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

| GPU | ëª¨ë¸ | ì¶”ë¡  ì†ë„ | ë©”ëª¨ë¦¬ | ë¹„ìš©/ì‹œê°„ |
|-----|------|----------|--------|----------|
| RTX 3090 | Qwen 7B | ~80 tok/s | 14GB | $0.24 |
| RTX 4090 | Qwen 7B | ~100 tok/s | 14GB | $0.40 |
| A100 40GB | Qwen 14B | ~90 tok/s | 28GB | $1.10 |
| A100 80GB | Qwen 32B | ~70 tok/s | 50GB | $1.89 |

---

## ğŸ†š Serverless vs Pod + Docker

| í•­ëª© | Serverless | Pod + Docker |
|------|-----------|-------------|
| **ì„¤ì •** | ê°„ë‹¨ | ì¤‘ê°„ |
| **ê´€ë¦¬** | ìë™ | ìˆ˜ë™ |
| **ì„±ëŠ¥** | ê°€ë³€ (Cold start) | **ê³ ì •** âœ… |
| **í™•ì¥** | ìë™ | ìˆ˜ë™ |
| **ë¹„ìš©** | ì‚¬ìš©ëŸ‰ ê¸°ì¤€ | ì‹œê°„ ê¸°ì¤€ |
| **ì œì–´** | ì œí•œì  | **ì™„ì „** âœ… |
| **ê¶Œì¥ ìš©ë„** | ë³€ë™ íŠ¸ë˜í”½ | **ì•ˆì •ì  ì„œë¹„ìŠ¤** âœ… |

---

## ğŸ“ ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- [vLLM Docker ê³µì‹ ë¬¸ì„œ](https://docs.vllm.ai/en/v0.8.0/deployment/docker.html)
- [RunPod ë¬¸ì„œ](https://docs.runpod.io/)
- [Docker Compose ë¬¸ì„œ](https://docs.docker.com/compose/)

---

**ì´ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ì€ ê²½ìš°ì— ì í•©í•©ë‹ˆë‹¤:**
- âœ… 24ì‹œê°„ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ í•„ìš”
- âœ… ì™„ì „í•œ ì œì–´ê¶Œ í•„ìš”
- âœ… Docker í™˜ê²½ ì»¤ìŠ¤í„°ë§ˆì´ì§• í•„ìš”
- âœ… ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥ í•„ìš”

**Serverlessê°€ ë” ë‚˜ì€ ê²½ìš°:**
- ê°„í—ì  ì‚¬ìš© (ë¹„ìš© ì ˆê°)
- íŠ¸ë˜í”½ ë³€ë™ì´ í° ê²½ìš°
- ìë™ í™•ì¥ í•„ìš”

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-05
**í…ŒìŠ¤íŠ¸ í™˜ê²½:** RunPod Community Cloud, RTX 3090
